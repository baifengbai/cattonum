---
title: "Introduction to cattonum"
author: "Bernie Gray"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to cattonum}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Background

## Case Study Setup

We'll demonstrate how to use `cattonum` by predicting flight delays (`dep_delay`) in the the `flights` dataset from the `nycflights13` package using random forests built with `ranger`.

```{r}
library(nycflights13)
library(ranger)
library(cattonum)
suppressPackageStartupMessages(library(dplyr))

set.seed(4444)

data(flights)
str(flights)
```

There are a lot of flights here and we don't want our model training to take forever, so let's only take a subset of these observations.  To simplify our analysis, we'll analyze only the three airlines with the most flights, only consider flights that were delayed in taking off, and remove some features.

```{r}
airlines_to_keep <- flights %>%
                      count(carrier) %>%
                      top_n(3, n) %>%
                      pull(carrier)

flights <- flights %>%
             filter(carrier %in% airlines_to_keep, dep_delay > 0) %>%
             select(-c(year, dep_time, sched_dep_time, arr_time, sched_arr_time,
                       arr_delay, flight, tailnum, time_hour))
str(flights)
```

In order to get more out of our time features, we'll do a quick transformation using the technique described [here](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/).  Also, `month` and `day` are currently integers, but they are really categorical, so we now turn them into characters (or factors, it doesn't matter for `cattonum`).

```{r}
tot_mins <- 24 * 60

flights <- flights %>%
             mutate(min_of_day = 60 * hour + minute,
                    cos_min_of_day = cos(2 * pi * min_of_day / tot_mins),
                    sin_min_of_day = cos(2 * pi * min_of_day / tot_mins)) %>%
             select(-c(min_of_day, hour, minute)) %>%
             mutate(month = as.character(month),
                    day = as.character(day)) %>%
             filter(complete.cases(.))
str(flights)
```

Now we turn to encoding our categorical features.  Consider a comparison between label encoding, mean encoding, and a mix of frequency, label, and mean encoding.  There are a few key points to note about the `catto_*` functions.

* They are designed to work in `dplyr`-style pipelines using `%>%` from `magrittr`.
*   If the function has encoded all factor and character columns, it will return a `matrix`; otherwise it will return either a `data.frame` or a `tibble`.
* They can handle data in a `data.frame` or `tibble`, and features can be specified in many different ways like in `dplyr`.  For example, the following are all equivalent for a data frame named `dat` with columns `x1` and `x2`.

```{r, eval = FALSE}
catto_label(dat)
catto_label(dat, x1, x2)
catto_label(dat, c(x1, x2))
catto_label(dat, c("x1", "x2"))
catto_label(dat, one_of(c("x1", "x2"))) # one_of is exported by dplyr
catto_label(dat, one_of("x1", "x2"))
```

Here we make the encoded datasets.

```{r}
label_encoded <- flights %>%
                   catto_label() # result is a matrix

mean_encoded <- flights %>%
                  catto_mean(response = dep_delay) # result is a matrix

mix_encoded <- flights %>%
                 catto_freq(dest) %>% # result is still a tibble
                 catto_label(origin) %>% # result is still a tibble
                 catto_mean(response = dep_delay) # result is a matrix
```

Now we can finally build the models.  We define a short function `get_oob_error` that builds an untuned random forest and returns the out-of-bag error.

```{r}
encodings <- list(label = label_encoded,
                  mean = mean_encoded,
                  mix = mix_encoded)

get_oob_error <- function(dat) {
  rf <- ranger(data = dat,
               num.trees = 100,
               dependent.variable.name = "dep_delay")
  rf$prediction.error
}

lapply(encodings, get_oob_error)
```

Mean encoding gives us the lowest OOB error, followed by the mixed encodings and label encoding.  This modeling setup of simply looking at OOB score on untuned random forests of 100 trees is not really a fair comparison, but it demonstrates the basic features of `cattonum`.